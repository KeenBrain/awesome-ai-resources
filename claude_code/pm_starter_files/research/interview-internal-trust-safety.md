# Internal Interview: Trust & Safety Lead
**Date:** November 7, 2025
**Interviewer:** Kai (Product Research)
**Duration:** 71 minutes
**Name:** Jordan Ellis
**Role:** Trust & Safety Lead
**Tenure:** 1.5 years
**Team Size:** 3 people (Jordan + 2 analysts)

---

*[Recording starts, video call, Jordan in a home office with a whiteboard behind them covered in incident timelines]*

**Kai:** Jordan, thanks for making time. I want to understand the T&S perspective.

**Jordan:** I've been waiting for someone to ask. Honestly. I've been raising flags for 8 months and the response is always "we'll get to it" or "that's important but not urgent." So let me tell you what's urgent.

**Kai:** Please.

**Jordan:** September 14th, 2025. A deepfake. Someone used Voice Studio to clone a political candidate's voice — I won't say which one, you can find it — and created robocall recordings. They called thousands of people with a fake endorsement message. It went viral. TechCrunch wrote about it. Then a Congressional staffer sent us a letter.

**Kai:** Walk me through the timeline.

**Jordan:** Sure. September 12th: The robocall recordings start circulating on social media. Someone identifies the voice as AI-generated by analyzing the audio waveform. They trace it to Wavelength's voice signature.

September 13th: TechCrunch reaches out for comment. Aria panics. Board emergency call at 10pm. We draft a statement saying "Wavelength condemns the misuse of voice technology" but we don't have an answer for "how did this happen?"

September 14th: The article drops. "AI Voice Startup Wavelength Used to Create Political Deepfake." Our stock options lost value that day. Not technically — we're private — but you could feel it.

September 15th: Congressional staffer sends a letter asking about our verification and consent policies. We have to respond within 30 days.

September 17th: Aria does a Twitter thread about responsible AI. It gets ratio'd because someone replies with "then why don't you have a consent framework?"

**Kai:** What consent framework DO you have?

**Jordan:** *[bitter laugh]* When you clone a voice, you check a box that says "I confirm I have the right to clone this voice." That's it. A checkbox. No verification. No audio watermarking. No way to prove consent. No way to detect if the uploaded audio belongs to the person using the platform.

**Kai:** So someone could upload any public figure's audio and clone their voice?

**Jordan:** Yes. Right now, today, someone could download a presidential speech from YouTube, upload it to Voice Studio, and have a functional clone in 5 minutes. We have no technical controls to prevent this. The only barrier is that checkbox.

**Kai:** What should exist?

**Jordan:** At minimum: voice verification. The person being cloned should have to speak a specific passphrase to prove they're consenting in real-time. Similar to what banks do for voice authentication. We should also have audio watermarking — every generated clip should have an inaudible signature that can be detected as AI-generated. And we need a takedown process — if someone discovers their voice has been cloned without consent, we should be able to identify and remove it within hours.

**Kai:** Why don't we have these things?

**Jordan:** Because leadership treats Trust & Safety as a cost center, not a product feature. Every time I propose a consent framework, the response is "that adds friction to onboarding" or "our competitors don't require that." Which is true — ElevenLabs doesn't have great consent either. But being the same level of bad as our competitors is not a strategy.

*[Jordan stands up, writes on whiteboard]*

**Jordan:** Let me show you the numbers. Since I joined 18 months ago:

- Voice misuse reports: 147
- Confirmed unauthorized clones: 38
- Celebrity/public figure clones: 12
- Takedown requests from individuals: 23
- Legal threats related to voice cloning: 4
- Congressional inquiries: 1

And those are just the ones we KNOW about. How many unauthorized clones exist that nobody has reported? We have no way to audit.

**Kai:** That's alarming.

**Jordan:** It keeps me up at night. Literally. I have a Google Alert for "Wavelength deepfake" and I check it first thing every morning.

*[sits back down]*

**Jordan:** Here's what scares me most. Enterprise customers are now asking "can you guarantee our voices won't be misused?" Like, if a company deploys Voice Agents and creates a custom voice for their brand, can we guarantee that voice won't be cloned by someone else? The honest answer is no. We can't. And if that gets out — if a major enterprise customer's brand voice gets cloned for a scam — we're done. That's not a PR crisis. That's an existential event.

**Kai:** What do enterprise customers say when you tell them that?

**Jordan:** I don't. Because nobody asks me. Sales closes the deal, engineering builds the integration, and nobody loops in T&S until something goes wrong. I found out about the CallVault deployment by accident — I saw it in a Slack thread. 500 agents handling 15,000 calls a day and nobody asked "hey Jordan, what are the trust and safety implications?"

**Kai:** That's... a process problem.

**Jordan:** It's a culture problem. T&S is reactive, not proactive. We're the fire department, not the fire prevention office. And we have 3 people. Three. For a platform that generates millions of voice clips a month. We should have at least 10 — including a policy lead, a content moderator, a ML engineer for detection, and an incident response specialist.

**Kai:** Have you raised this with leadership?

**Jordan:** Multiple times. Aria says "absolutely, we'll hire." Dev says "we need to solve this with technology, not headcount." Ravi (VP Product) says "it's on the roadmap." Nobody actually allocates budget.

*[pause]*

**Jordan:** Can I be really honest with you?

**Kai:** Please.

**Jordan:** I'm thinking about leaving. Not because I don't care — because I care too much. I took this job because I believe voice AI needs responsible stewards. But I can't be a responsible steward with 3 people, no budget, no authority, and leadership that views my concerns as obstacles to growth.

Every time I raise a risk, I'm implicitly saying "this might slow us down." And in a startup culture that worships speed, saying "slow down" makes you the enemy. I'm not the enemy. The enemy is the next deepfake incident. And it WILL happen. It's not if, it's when.

**Kai:** What would make you stay?

**Jordan:** Three things. One: headcount. Give me 10 people and a real budget. Two: authority. T&S review should be mandatory before any new feature or product launch. Like a security review, but for trust and safety. Three: a consent framework. Real one. With voice verification, watermarking, and takedown capabilities. Ship it in Q1 or I'm gone by Q2.

**Kai:** Those are clear asks.

**Jordan:** I've written them up. Three times. The documents are in our wiki. Nobody reads them. Maybe if you put it in a research synthesis, someone will listen.

*[pause]*

**Jordan:** One more thing. The Soundscape music situation? That's a trust and safety problem too. Generating music that mimics specific artists is a misuse of the technology, even if it's not a deepfake of a person's voice. The same ethical framework applies. And the fact that it happened — that someone was able to create a song clearly mimicking a specific artist and it went viral before we caught it — proves that our content moderation is broken.

We don't have real-time content moderation for Soundscape. We don't have artist similarity detection. We don't have usage monitoring that would flag unusual patterns. Someone generated that viral song and we didn't know until it hit TikTok. That's unacceptable.

**Kai:** Thank you, Jordan. This is probably the most important interview I've done.

**Jordan:** I hope so. Because I'm running out of ways to say "this is urgent" without being heard.

*[recording ends]*

---

## Key Themes
1. **CRITICAL:** Political deepfake incident hit TechCrunch, Congressional inquiry
2. Voice consent framework is a checkbox — no real verification
3. 147 voice misuse reports, 38 confirmed unauthorized clones
4. Enterprise customers cannot be guaranteed voice security
5. T&S has 3 people — needs 10
6. Leadership treats T&S as cost center, not priority
7. Jordan is considering leaving — major retention risk
8. No content moderation for Soundscape (how the music incident happened)
9. No audio watermarking or provenance tracking
10. No proactive T&S review in product development process
11. "We're one incident away from regulation"
