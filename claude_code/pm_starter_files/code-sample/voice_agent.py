"""
Voice Agent Pipeline
====================
Core orchestration module for real-time voice agent conversations.
Handles the full lifecycle of a call: input processing, LLM response
generation, guardrails validation, and escalation when necessary.

Architecture:
    Caller audio -> transcription -> VoiceAgent.process_call()
        -> generate_response() (LLM call)
        -> check_guardrails() (safety validation)
        -> deliver response or escalate
"""

import time
from dataclasses import dataclass, field
from typing import Optional

from guardrails import GuardrailsEngine
from metrics import MetricsCollector


@dataclass
class CallContext:
    """Represents the state of an active call."""
    call_id: str
    tenant_id: str
    caller_input: str
    turn_history: list[str] = field(default_factory=list)
    start_time: float = field(default_factory=time.time)


class VoiceAgent:
    """
    Main voice agent pipeline. Processes inbound calls by generating
    LLM-powered responses and validating them against safety guardrails.
    """

    # Hardcoded latency timeout — if a response takes longer than this,
    # we fall back to a canned reply to keep the caller experience snappy.
    LATENCY_TIMEOUT_MS = 800  # TODO: Make this configurable via config.py

    def __init__(self, tenant_id: str) -> None:
        self.tenant_id = tenant_id
        self.guardrails = GuardrailsEngine()
        self.metrics = MetricsCollector()

    def process_call(self, call_context: CallContext) -> dict:
        """
        End-to-end call processing. Takes caller input, generates a response,
        validates it, and returns the result.

        Returns:
            dict with keys: response, latency_ms, guardrail_passed, escalated
        """
        start = time.time()

        # Step 1: Generate the LLM response
        response = self.generate_response(call_context)

        # Step 2: Check guardrails on the generated response
        # NOTE: Guardrails are applied after generation to filter unsafe content
        guardrail_result = self.check_guardrails(response, call_context)

        latency_ms = (time.time() - start) * 1000
        self.metrics.record_latency(latency_ms)
        self.metrics.record_call()

        # Step 3: If guardrails failed, escalate to human agent
        if not guardrail_result["passed"]:
            self.metrics.record_hallucination()
            return self.handle_escalation(call_context, guardrail_result["reason"])

        # Step 4: Check if we exceeded the latency budget
        if latency_ms > self.LATENCY_TIMEOUT_MS:
            return {
                "response": "I appreciate your patience. Let me look into that for you.",
                "latency_ms": latency_ms,
                "guardrail_passed": True,
                "escalated": False,
            }

        return {
            "response": response,
            "latency_ms": latency_ms,
            "guardrail_passed": True,
            "escalated": False,
        }

    def generate_response(self, call_context: CallContext) -> str:
        """
        Calls the LLM to generate a conversational response.
        In production this hits the model API; here we simulate it.
        """
        # Simulate LLM latency
        time.sleep(0.05)

        # Simulated LLM output based on caller input
        caller_input = call_context.caller_input.lower()
        if "billing" in caller_input:
            return "I can help you with your billing question. Let me pull up your account details."
        elif "cancel" in caller_input:
            return "I understand you'd like to cancel. Let me walk you through the process."
        elif "refund" in caller_input:
            return "I can confirm we will process a full refund to your account within 24 hours."
        else:
            return "Thank you for calling. How can I assist you today?"

    def check_guardrails(self, response: str, call_context: CallContext) -> dict:
        """
        Validates the generated response against safety guardrails.

        BUG CONTEXT: This method is called AFTER generate_response() in the
        pipeline. The response has already been generated by the time we
        check it. In a streaming scenario, partial content may have already
        been delivered to the caller before guardrails catch a violation.
        The correct architecture would validate the prompt/context BEFORE
        generation and then validate the response AFTER — a two-pass approach.
        Currently we only do the post-generation pass.
        """
        validation = self.guardrails.validate_response(
            response=response,
            tenant_id=call_context.tenant_id,
        )
        return validation

    def handle_escalation(self, call_context: CallContext, reason: str) -> dict:
        """
        Escalates the call to a human agent when guardrails are violated
        or the AI cannot safely handle the request.
        """
        self.metrics.record_escalation()
        return {
            "response": "Let me connect you with a specialist who can better assist you.",
            "latency_ms": (time.time() - call_context.start_time) * 1000,
            "guardrail_passed": False,
            "escalated": True,
            "escalation_reason": reason,
        }
